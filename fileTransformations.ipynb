{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d24d164e",
   "metadata": {},
   "source": [
    "# File Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff1edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import dask.dataframe as dd\n",
    "import dask.array as dsa\n",
    "import zarr\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import intake\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc34806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be needed to write data into your bucket if it does not have public write access\n",
    "token = '/path/to/your/token.json' \n",
    "\n",
    "# Bucket name/public URL that contains the data you would like to convert\n",
    "root = 'gs://your/bucket/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488867d2",
   "metadata": {},
   "source": [
    "Note: The `name_function` does not sort partitions in the output files. Therefore, when using this method to split up CSV files into partitions of the same (or different) file type, make sure to include a sorting feature in the naming function.\n",
    "\n",
    "In this instance, since these files will be used to measure read speed, the order that the files are concatenated by Dask when they are called into the timing program does not matter. If this method is being used for machine learning or data analysis, it might be a good idea to preserve the partition order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1a4961",
   "metadata": {},
   "source": [
    "## Timing Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1000da1a",
   "metadata": {},
   "source": [
    "We will be using the same diagnostic timer as seen in the `transferSpeeds.ipynb` notebook. In this case, it will keep track of how long the file conversion process takes. Note that some files will have to be loaded locally in order to convert, given the limitations of the python libraries used to facilitate the conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cbcc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagnosticTimer:\n",
    "    def __init__(self):\n",
    "        self.diagnostics = []\n",
    "        \n",
    "    @contextmanager\n",
    "    def time(self, **kwargs):\n",
    "        tic = time.time()\n",
    "        yield\n",
    "        toc = time.time()\n",
    "        kwargs[\"runtime\"] = toc - tic\n",
    "        self.diagnostics.append(kwargs)\n",
    "        \n",
    "    def dataframe(self):\n",
    "        return pd.DataFrame(self.diagnostics)\n",
    "    \n",
    "diag_timer = DiagnosticTimer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef32607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names to give CSV columns. If the file does not have column names, Dask/Pandas will use your first line of data as such.\n",
    "names=['lon', 'lat', 'z']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101899e7",
   "metadata": {},
   "source": [
    "## CSV to Partitioned Parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe9376",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_function = lambda x: f\"ETOPO1_Ice_g_gmt4_{x}.parquet\"\n",
    "\n",
    "with diag_timer.time(conversionType='csv2partparqet'):\n",
    "    df = dd.read_csv(root + 'ETOPO1_Ice_g_gmt4.csv', assume_missing=True, header=None, names=names)\n",
    "    dd.to_parquet(df, root + 'parquetpartitions', name_function=name_function, \n",
    "                  storage_options={'token':token})\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ca403d",
   "metadata": {},
   "source": [
    "## CSV to One Parquet File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6801c2",
   "metadata": {},
   "source": [
    "Note that using this method requires that the CSV and output Parquet file are stored in a local disk. You cannot read and write directly from cloud storage using Pandas. The time this cell takes to execute is not representative of the total time it would take to copy the file from cloud storage onto a local disk, convert the file, and move back onto cloud storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with diag_timer.time(conversionType='csv2parquet_local'):\n",
    "    # Replace path with your own local file path. Ensure that an appropriate engine is installed within your environment.\n",
    "    df = pd.read_csv('/local/file/path/ETOPO1_Ice_g_gmt4.csv', header=None, names=names)\n",
    "    df.to_parquet('/local/file/path/ETOPO1_Ice_g_gmt4.parquet', engine='fastparquet')\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db10fb6e",
   "metadata": {},
   "source": [
    "## CSV to Partitioned CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53adfa7f",
   "metadata": {},
   "source": [
    "The `header_first_partition_only=True` argument is very important in this instance, otherwise your header line will be written to each output file. To keep only your data, also make sure that no header options are included in the `dd.read_csv(...)` line, including `header=None` & `names=[...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bbf8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_function(i):\n",
    "    return \"ETOPO1_Ice_g_gmt4_\" + str(i) + \".csv\"\n",
    "\n",
    "with diag_timer.time(conversionType='csv2partcsv'):\n",
    "    df = dd.read_csv(root + 'ETOPO1_Ice_g_gmt4.csv', assume_missing=True)\n",
    "    dd.to_csv(df, root + 'csvpartitions', name_function=name_function, \n",
    "              storage_options={'token':token}, header_first_partition_only=True)\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb22a2d",
   "metadata": {},
   "source": [
    "## NetCDF to Zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495d4c0f",
   "metadata": {},
   "source": [
    "For multi-variable gridded data, you will need to create `n` Zarr arrays for `n` variables, whereas a Zarr Group will be able to incorporate all variables in a single parent directory in object storage using `Xarray.DataSet.to_zarr(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d16f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.open_netcdf(root + 'ETOPO1_Ice_g_gmt4.nc').to_dask().data_vars \n",
    "# Lists all data variables contained in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9881692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'Z1' # Choose data variable to convert into Zarr Array & Group. If you have multiple data variables\n",
    "                 # they need to be chunked separately and put into the same DataSet to be converted into a Zarr Group.\n",
    "    \n",
    "# variable2 = ['Your Variable Name']  Add as many variables as your data set contains    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab6609",
   "metadata": {},
   "source": [
    "### Zarr Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12c2b30",
   "metadata": {},
   "source": [
    "This approach uses Xarray to store the contents of the NetCDF file within a Zarr group. Note that there is no method of retrieving the NetCDF file directly from cloud storage. Writing consolidated metadata is recommended for maximum read speedup.\n",
    "\n",
    "Selecting the correct chunk sizes is very important to get efficient read speed, and I have found that preserving the internal chunk sizes of the original NetCDF4 data allows for the best data retieval speeds when being accessed from Zarr formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462dd304",
   "metadata": {},
   "outputs": [],
   "source": [
    "with diag_timer.time(conversionType='netcdf2zgroup'):\n",
    "    ds = intake.open_netcdf(root + 'ETOPO1_Ice_g_gmt4.nc').to_dask()\n",
    "    da = ds[variable]\n",
    "    # da2 = ds[variable2]  Can be increased to N variables\n",
    "    internal_chunks = da.encoding['chunksizes']\n",
    "    coords = da.dims\n",
    "    \n",
    "    da = da.chunk(chunks=dict(zip(coords, internal_chunks)))\n",
    "    ds = da.to_dataset() # Will need to be altered if you have multiple variables\n",
    "    ds.to_zarr(store= root + 'ETOPO1_Ice_g_gmt4.zarr', storage_options={'token':token}, \n",
    "               consolidated=True)\n",
    "del ds, da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe657c4b",
   "metadata": {},
   "source": [
    "### Zarr Array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae21af1",
   "metadata": {},
   "source": [
    "When converting from a Xarray DataSet to Xarray DataArray, note that you can only select one variable at a time. If you are working with data that has multiple data variables, you must convert each variable to a separate array. So, for `n` data variables you must execute the below cell `n` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0f8c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with diag_timer.time(conversionType='netcdf2zarray'):\n",
    "    ds = intake.open_netcdf(root + 'ETOPO1_Ice_g_gmt4.nc').to_dask()\n",
    "    da = ds[variable].chunk(chunks=dict(zip(coords, internal_chunks))).data # Change the variable name as needed\n",
    "    dsa.to_zarr(da, root + 'ETOPO1_Ice_g_gmt4.zarray', storage_options={'token':token})\n",
    "del ds, da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d17a8d2",
   "metadata": {},
   "source": [
    "## Present Timing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = diag_timer.dataframe()\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH ubuntu@10.128.0.150 jacob-lab-jgreen_daskMLtut",
   "language": "",
   "name": "rik_ssh_ubuntu_10_128_0_150_jacoblabjgreen_daskmltut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
