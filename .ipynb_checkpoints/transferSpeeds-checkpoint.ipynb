{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Parallel Data Read Speeds with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This benchmarking was inspired by the [Abernathey et al. (2021)](https://www.computer.org/csdl/magazine/cs/2021/02/09354557/1reXu4gJjri) paper, which includes a detailed analysis and discussion of cloud-native big scientific data. The paper is much wider in scope than this benchmarking, so it is recommended that a reader unacclimated with this subject matter first studies its information before going through this notebook.\n",
    "\n",
    "Unfortunately, due to the access token required to retrieve data the Google Cloud Storage Bucket used in this benchmarking, the public reader will not be able to run this notebook on their own. If you wish to download your own copy and alter the code, the test data from [NOAA's ETOPO1 Global Relief Model](https://www.ngdc.noaa.gov/mgg/global/relief/ETOPO1/data/ice_surface/grid_registered/netcdf/) can be found here. A file transformation notebook is also found within the repository, which documents my methods for converting from the original NetCDF format. Note that a CSV version was created outside of this project space.\n",
    "\n",
    "Different methods for accessing Google Cloud Storage are used--contigent on the file type of the data. Though discussed in further detail later on, I encourage the reader to read about the different file formats used in this benchmarking, as well as the access APIs. In many instances, data can be loaded directly into Dask objects without further manipulation. Other libraries were also used to load data in, mainly due to the limitations of Dask or speed benefits of using another library.\n",
    "\n",
    "A significant portion of the code used in this benchmarking is directly taken from [Ryan Abernathey's demonstration notebook](https://github.com/earthcube2020/ec20_abernathey_etal/blob/master/cloud_storage.ipynb), and it is worth viewing before beginning this read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Client Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as dsa\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from contextlib import contextmanager\n",
    "import xarray as xr\n",
    "import intake\n",
    "import time\n",
    "import dask\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import pandas as pd\n",
    "import gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create this null storage object. To measure our throughput, all of the data will need to be accessed at a single time and can be achieved by storing the data into this null storage target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevNullStore:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __setitem__(*args, **kwargs):\n",
    "        pass\n",
    "\n",
    "null_store = DevNullStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Diagnostic Timer will keep track of data retrieval times and store them within a pandas dataframe for later processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagnosticTimer:\n",
    "    def __init__(self):\n",
    "        self.diagnostics = []\n",
    "        self.names = []\n",
    "        \n",
    "    @contextmanager\n",
    "    def time(self, **kwargs):\n",
    "        tic = time.time()\n",
    "        yield\n",
    "        toc = time.time()\n",
    "        kwargs[\"runtime\"] = toc - tic\n",
    "        self.diagnostics.append(kwargs)\n",
    "        \n",
    "    def dataframe(self):\n",
    "        return pd.DataFrame(self.diagnostics)\n",
    "    \n",
    "diag_timer = DiagnosticTimer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This naming function will keep track of our read throughput for each file type and make it easier to plot all of the cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name(fileType, daf): # Takes a string fileType input & dataframe type input for daf \n",
    "    globals()[f\"df_{fileType}\"] = daf\n",
    "    diag_timer.names.append(globals()[f\"df_{fileType}\"])\n",
    "    \n",
    "    global df, da\n",
    "    del df, da\n",
    "    \n",
    "    diag_timer.diagnostics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we are accessing the data from different formats, the core process will be the exact same. Dask is lazy by default, which means we will be using the previously defined null_store to measure throughput. Converting from DataFrames to Arrays, for example, will not affect the access speed because the data is not actually read from the source until we \"store\" it. So, these read speeds should be the same whether you are initially pointing to data using a dataframe or array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A token may or may not be needed depending on the permissions your specific Google Bucket requires.\n",
    "token = '/home/ubuntu/Cloud-Data-Transfer-Speed-Benchmarks/cloud-data-benchmarks.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about our cluster is also collected during the tests. Importantly, the number of parallel reads will correspond to the total number of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_nthreads():\n",
    "    return sum([v for v in client.nthreads().values()])\n",
    "\n",
    "def total_ncores():\n",
    "    return sum([v for v in client.ncores().values()])\n",
    "\n",
    "def total_workers():\n",
    "    return len(client.ncores())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main loop. A few modifications have been made from Abernathey's original version, but most notably we want to measure the connection time to the data reference. This shouldn't matter if we are purely measuring access times, but it is important when we use different modules to connect to Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainLoop(da, diag_kwargs):\n",
    "    for nworkers in [5, 4, 3]:\n",
    "        cluster.scale(nworkers)\n",
    "        time.sleep(10)\n",
    "        client.wait_for_workers(nworkers)\n",
    "        print(nworkers)\n",
    "        with diag_timer.time(nthreads=total_nthreads(), ncores=total_ncores(), nworkers=total_workers(), **diag_kwargs):\n",
    "            future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "            dask.compute(future, retries=5)\n",
    "            \n",
    "    df = diag_timer.dataframe()\n",
    "    df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the classic tabular data format. Students & professionals alike use this format due to its simplicity and flexibility bewteen applications. As you will see, this format has the slowest read times of all being tested but is included in the benchmarking given its widespread use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 5.21 GiB </td>\n",
       "                        <td> 54.47 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (233312400, 3) </td>\n",
       "                        <td> (2379761, 3) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 208 Tasks </td>\n",
       "                        <td> 104 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float64 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"5\" x2=\"25\" y2=\"5\" />\n",
       "  <line x1=\"0\" y1=\"11\" x2=\"25\" y2=\"11\" />\n",
       "  <line x1=\"0\" y1=\"19\" x2=\"25\" y2=\"19\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"25\" y2=\"25\" />\n",
       "  <line x1=\"0\" y1=\"32\" x2=\"25\" y2=\"32\" />\n",
       "  <line x1=\"0\" y1=\"38\" x2=\"25\" y2=\"38\" />\n",
       "  <line x1=\"0\" y1=\"45\" x2=\"25\" y2=\"45\" />\n",
       "  <line x1=\"0\" y1=\"50\" x2=\"25\" y2=\"50\" />\n",
       "  <line x1=\"0\" y1=\"58\" x2=\"25\" y2=\"58\" />\n",
       "  <line x1=\"0\" y1=\"63\" x2=\"25\" y2=\"63\" />\n",
       "  <line x1=\"0\" y1=\"70\" x2=\"25\" y2=\"70\" />\n",
       "  <line x1=\"0\" y1=\"76\" x2=\"25\" y2=\"76\" />\n",
       "  <line x1=\"0\" y1=\"82\" x2=\"25\" y2=\"82\" />\n",
       "  <line x1=\"0\" y1=\"88\" x2=\"25\" y2=\"88\" />\n",
       "  <line x1=\"0\" y1=\"95\" x2=\"25\" y2=\"95\" />\n",
       "  <line x1=\"0\" y1=\"100\" x2=\"25\" y2=\"100\" />\n",
       "  <line x1=\"0\" y1=\"107\" x2=\"25\" y2=\"107\" />\n",
       "  <line x1=\"0\" y1=\"113\" x2=\"25\" y2=\"113\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,120.0 0.0,120.0\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >3</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">233312400</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<values, shape=(233312400, 3), dtype=float64, chunksize=(2379761, 3), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_csv('gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.csv', assume_missing=True)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='CSV', connectTime=connectTime)\n",
    "df = mainLoop(da, diag_kwargs)\n",
    "name('csv', df)\n",
    "df_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While one CSV file is the most common way one will see data of this format presented, we are interested in determining if partitioning (splitting up) the data into file sizes automatically determined by Dask will speed up the read from cloud storage. The process is the exact same, with the exception of a minor amount of required preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_csv('gs://cloud-data-benchmarks/csvpartitions/*', assume_missing=True)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='Partitioned CSV', connectTime=connectTime)\n",
    "df = mainLoop(da, diag_kwargs)\n",
    "name('partcsv', df)\n",
    "df_partcsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetCDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NetCDF files are extremely common in geospatial and climate science, which is the original format of the test data used in this benchmarking. Data from these fields are commonly very large, with sizes only increasing as missions collecting samples at higher resolutions are conducted. The informed scientist should have a good grasp on the speed limitations of this format and the best ways to load in and perform computations within Python.\n",
    "\n",
    "We load the NetCDF using the intake library. This is a great way to load the data from GCS without directly using the gcsfs module. Depending on the NetCDF file contents, decoding the data using the engines included with xarray.open_dataset(...) can be troublesome. Read the XArray documentation about accessing files using xarray.open_dataset(...) [here](https://docs.xarray.dev/en/stable/user-guide/io.html).\n",
    "\n",
    "The intake module is much easier to use, and offers some additional functionality in the form of catalogs. Read about catalogs and their uses [here](https://intake.readthedocs.io/en/latest/catalog.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "data = intake.open_netcdf('gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.nc').to_dask()\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "data = data.to_array()\n",
    "da = data.data # Retrieves raw values from wrapped Xarray DataArray object\n",
    "da = dsa.from_array(da)\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "del data\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an alternate method for loading a NetCDF file into a Dask array. When running this notebook, choose only one of these methods. Otherwise, you will likely encounter very high memory usage & overwrite the data references in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='NetCDF', connectTime=connectTime)\n",
    "df = mainLoop(da, diag_kwargs)\n",
    "name('netcdf', df)\n",
    "df_netcdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughput will be tested in two different ways:\n",
    "* A CSV file was partitioned into a DataFrame by Dask, and 1 parquet file was written per DataFrame partition. All of these files will be read into the null storage target\n",
    "* A single parquet file containing all of the original CSV information will be read into the null storage target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_parquet(\"gs://cloud-data-benchmarks/parquetpartitions/*\")\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='Partitioned Parquet', connectTime=connectTime)\n",
    "df = mainLoop(da, diag_kwargs)\n",
    "name('partparquet', df)\n",
    "df_partparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encounter a problem with large unmanaged memory when attempting to compute the size of DataFrame. Generally, it is recommended that parquet files be partitioned in advance to avoid this issue. This will likely be remedied when memory resources are increased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_parquet(\"gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.parquet\")\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "\n",
    "# Memory Problem between this print statements, similar to the NetCDF issue\n",
    "print(\"Calculating size of array\")\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "print(\"Size of array has been calculated\")\n",
    "\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='Parquet', connectTime=connectTime)\n",
    "df = mainLoop(da, diag_kwargs)\n",
    "name('parquet', df)\n",
    "df_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See <a href=\"https://www.programcreek.com/python/example/128207/dask.array.from_zarr#:~:text=.chunks)-,Example%205,-Project%3A%20napari\">this python example</a> of dask.array.from_zarr(...) for further understanding of the differences between a Zarr Array & Zarr Group.\n",
    "\n",
    "A Zarr Group is a very good choice of storage utility for multiscale data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zarr Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "da = dsa.from_zarr('gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.zarray')\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='Zarr Array', connectTime=connectTime)\n",
    "df = mainLoop(da, diag_kwargs)\n",
    "name('zarray', df)\n",
    "df_zarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zarr Hierachical Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xarray provides a fantasic tool in their API for opening Zarr groups. By consolidating the metadata upon creation of the group, the read speed increases considerably more than if it was left in its base state. In a future update, the chunk size of the Zarr Group will be decreased because the overhead associated with such small chunks is far too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "zarr_ds = xr.open_zarr(store='gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.zarr', consolidated=True)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "darray = zarr_ds.to_array()\n",
    "da = darray.data\n",
    "\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point in time, throughput results of the Zarr Group are not accurate due to the extremely small chunk size. A speedup is expected once the group is stored in smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='Zarr Group', connectTime=connectTime)\n",
    "df = mainLoop(da, diag_kwargs)\n",
    "name('zgroup', df)\n",
    "df_zgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = cm.rainbow(np.linspace(0,1,len(diag_timer.names)))\n",
    "legend = []\n",
    "\n",
    "for i in range(len(diag_timer.names)):\n",
    "    legend.append(diag_timer.names[i]['format'][1])\n",
    "    c = color[i,:]\n",
    "    \n",
    "    if i==0:\n",
    "        ax = diag_timer.names[i].plot(x='nworkers', y='throughput_Mbps', title='Cloud Data Read Speeds with Dask',\n",
    "                                      kind='scatter', color=c)\n",
    "    \n",
    "    elif i==(len(diag_timer.names)-1):\n",
    "        diag_timer.names[i].plot(x='nworkers', y='throughput_Mbps', kind='scatter', color=c, ax=ax, \n",
    "                                 xlabel='Number of Parallel Reads', ylabel='Throughput (Mbps)')\n",
    "        \n",
    "        plt.grid(True)\n",
    "        plt.legend(legend, bbox_to_anchor=[1.25, 0.5],loc='center', title='Store Formats')\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        #plt.yscale('symlog') ACTIVATE THIS LINE IF YOU ARE USING A LARGE AMOUNT OF WORKERS\n",
    "    \n",
    "    else:\n",
    "        diag_timer.names[i].plot(x='nworkers', y='throughput_Mbps', kind='scatter', color=c, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH ubuntu@10.128.0.150 jacob-lab-jgreen_daskMLtut",
   "language": "",
   "name": "rik_ssh_ubuntu_10_128_0_150_jacoblabjgreen_daskmltut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
