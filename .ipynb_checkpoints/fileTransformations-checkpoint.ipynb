{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edec9827",
   "metadata": {},
   "source": [
    "# File Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b9082c",
   "metadata": {},
   "source": [
    "## For Use in Cloud Data Throughput Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2418e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import dask.dataframe as dd\n",
    "import dask.array as dsa\n",
    "import zarr\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import intake\n",
    "from contextlib import contextmanager\n",
    "import gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b650190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = '/path/to/token.json' # This will be needed to write data into your bucket if it does not have public write access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf4faa",
   "metadata": {},
   "source": [
    "Note: The name_function does not sort partitions in the output files. Therefore, when using this method to split up CSV files into partitions of the same (or different) file type, make sure to include a sorting feature in the naming function.\n",
    "\n",
    "In this instance, since these files will be used to measure read speed, the order that the files are concatenated by Dask when they are called into the timing program does not matter. If this method is being used for machine learning or data analysis, it might be a good idea to preserve the partition order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2c6b36",
   "metadata": {},
   "source": [
    "## Timing Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f624a09",
   "metadata": {},
   "source": [
    "We will be using the same diagnostic timer as seen in the Transfer Speeds notebook. In this case, it will keep track of how long the file conversion process takes. Note that some files will have to be loaded locally in order to convert, given the limitations of the python libraries used to facilitate the conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b1d0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagnosticTimer:\n",
    "    def __init__(self):\n",
    "        self.diagnostics = []\n",
    "        \n",
    "    @contextmanager\n",
    "    def time(self, **kwargs):\n",
    "        tic = time.time()\n",
    "        yield\n",
    "        toc = time.time()\n",
    "        kwargs[\"runtime\"] = toc - tic\n",
    "        self.diagnostics.append(kwargs)\n",
    "        \n",
    "    def dataframe(self):\n",
    "        return pd.DataFrame(self.diagnostics)\n",
    "    \n",
    "diag_timer = DiagnosticTimer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41045ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names to give CSV columns. If the file does not have column names, Dask/Pandas will use your first line of data as such.\n",
    "names=['lon', 'lat', 'z']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b077a85",
   "metadata": {},
   "source": [
    "## CSV to Partitioned Parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8bbd2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_function = lambda x: f\"ETOPO1_Ice_g_gmt4_{x}.parquet\"\n",
    "\n",
    "with diag_timer.time(conversionType='csv2partparqet'):\n",
    "    df = dd.read_csv('gs://path/to/bucket/ETOPO1_Ice_g_gmt4.csv', assume_missing=True, header=None, names=names)\n",
    "    dd.to_parquet(df, 'gs://path/to/bucket/parquetpartitions', name_function=name_function, \n",
    "                  storage_options={'token':token})\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed875a",
   "metadata": {},
   "source": [
    "## CSV to One Parquet File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d1480",
   "metadata": {},
   "source": [
    "Note that using this method requires that the CSV and output Parquet file are stored in a local disk. You cannot read and write directly from cloud storage using Pandas. Therefore, the time it takes to read and write this file will not be accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea8b4a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "with diag_timer.time(conversionType='csv2parquet_local'):\n",
    "    # Replace path with your own local file path. Ensure that an appropriate engine is installed within your environment.\n",
    "    df = pd.read_csv('/local/file/path/ETOPO1_Ice_g_gmt4.csv', header=None, names=names)\n",
    "    df.to_parquet('/local/file/path/ETOPO1_Ice_g_gmt4.parquet', engine='fastparquet')\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428670b3",
   "metadata": {},
   "source": [
    "## CSV to Partitioned CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5b04bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In order to preserve order between partitions paths created with ``name_function`` should sort to partition order\n"
     ]
    }
   ],
   "source": [
    "def name_function(i):\n",
    "    return \"ETOPO1_Ice_g_gmt4_\" + str(i) + \".csv\"\n",
    "\n",
    "with diag_timer.time(conversionType='csv2partcsv'):\n",
    "    df = dd.read_csv('gs://path/to/bucket/ETOPO1_Ice_g_gmt4.csv', assume_missing=True, header=None, names=names)\n",
    "    dd.to_csv(df, 'gs://path/to/bucket/csvpartitions', name_function=name_function, storage_options={'token':token})\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1af2a2",
   "metadata": {},
   "source": [
    "## NetCDF to Zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63639a33",
   "metadata": {},
   "source": [
    "### Zarr Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29941be",
   "metadata": {},
   "source": [
    "This approach uses Xarray to store the contents of the NetCDF file within a Zarr group. Note that there is no method of retrieving the NetCDF file directly from cloud storage. Writing consolidated metadata is recommended for maximum read speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bea7535",
   "metadata": {},
   "outputs": [],
   "source": [
    "with diag_timer.time(conversionType='netcdf2zgroup'):\n",
    "    ds = intake.open_netcdf('gs://path/to/bucket/ETOPO1_Ice_g_gmt4.nc').to_dask()\n",
    "    ds.to_zarr(store='gs://path/to/bucket/ETOPO1_Ice_g_gmt4.zarr', storage_options={'token':token}, \n",
    "               consolidated=True)\n",
    "del ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efa3c6f",
   "metadata": {},
   "source": [
    "### Zarr Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ee4bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "with diag_timer.time(conversionType='netcdf2zarray'):\n",
    "    ds = intake.open_netcdf('gs://path/to/bucket/ETOPO1_Ice_g_gmt4.nc').to_dask()\n",
    "    darray = ds.to_array()\n",
    "    data = darray.data\n",
    "    da = dsa.from_array(data)\n",
    "    dsa.to_zarr(da, 'gs://path/to/bucket/ETOPO1_Ice_g_gmt4.zarray', storage_options={'token':token})\n",
    "del ds, darray, data, da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fded6e",
   "metadata": {},
   "source": [
    "## Present Timing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66386905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversionType</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>csv2partparqet</td>\n",
       "      <td>66.089746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>csv2parquet_local</td>\n",
       "      <td>125.892774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>csv2partcsv</td>\n",
       "      <td>909.665932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>netcdf2zgroup</td>\n",
       "      <td>82.143818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>netcdf2zarray</td>\n",
       "      <td>24.640987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      conversionType     runtime\n",
       "0     csv2partparqet   66.089746\n",
       "1  csv2parquet_local  125.892774\n",
       "2        csv2partcsv  909.665932\n",
       "3      netcdf2zgroup   82.143818\n",
       "4      netcdf2zarray   24.640987"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = diag_timer.dataframe()\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH ubuntu@10.128.0.150 jacob-lab-jgreen_daskMLtut",
   "language": "",
   "name": "rik_ssh_ubuntu_10_128_0_150_jacoblabjgreen_daskmltut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
