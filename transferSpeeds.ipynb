{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Parallel Data Read Speeds with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Client Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as dsa\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from contextlib import contextmanager\n",
    "import intake\n",
    "import time\n",
    "import dask\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/ubuntu/dask-worker-space/worker-uiw8cpiu', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/ubuntu/dask-worker-space/worker-uk4tq3ws', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/ubuntu/dask-worker-space/worker-udq4p0di', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/ubuntu/dask-worker-space/worker-zhl_bg79', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/ubuntu/dask-worker-space/worker-em0h4d8m', purging\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create this null storage object (GitHub Abernathy, Ryan). To measure our throughput, all the data will need to be accessed at a single time, which can be achieved by storing the data into this null storage target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevNullStore:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __setitem__(*args, **kwargs):\n",
    "        pass\n",
    "\n",
    "null_store = DevNullStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is taken directly from GitHub\n",
    "Abernathy, Ryan, \n",
    "\n",
    "The Diagnostic Timer will keep track of data retrieval times and store them within a pandas dataframe for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagnosticTimer:\n",
    "    def __init__(self):\n",
    "        self.diagnostics = []\n",
    "        \n",
    "    @contextmanager\n",
    "    def time(self, **kwargs):\n",
    "        tic = time.time()\n",
    "        yield\n",
    "        toc = time.time()\n",
    "        kwargs[\"runtime\"] = toc - tic\n",
    "        self.diagnostics.append(kwargs)\n",
    "        \n",
    "    def dataframe(self):\n",
    "        return pd.DataFrame(self.diagnostics)\n",
    "    \n",
    "diag_timer = DiagnosticTimer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we are accessing the data from different formats, the core process will be the exact same. Dask is lazy by default, which means we will be using the previously defined null_store to measure throughput. Converting from DataFrames to Arrays, for example, will not affect the access speed because the data is not actually read from the source until we \"store\" it. So, these read speed should be the same whether you are using a dataframe or array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = '/home/ubuntu/Cloud-Data-Transfer-Speed-Benchmarks/cloud-data-benchmarks.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_nthreads():\n",
    "    return sum([v for v in client.nthreads().values()])\n",
    "\n",
    "def total_ncores():\n",
    "    return sum([v for v in client.ncores().values()])\n",
    "\n",
    "def total_workers():\n",
    "    return len(client.ncores())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 5.21 GiB </td>\n",
       "                        <td> 54.47 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (233312400, 3) </td>\n",
       "                        <td> (2379761, 3) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 208 Tasks </td>\n",
       "                        <td> 104 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float64 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"5\" x2=\"25\" y2=\"5\" />\n",
       "  <line x1=\"0\" y1=\"11\" x2=\"25\" y2=\"11\" />\n",
       "  <line x1=\"0\" y1=\"19\" x2=\"25\" y2=\"19\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"25\" y2=\"25\" />\n",
       "  <line x1=\"0\" y1=\"32\" x2=\"25\" y2=\"32\" />\n",
       "  <line x1=\"0\" y1=\"38\" x2=\"25\" y2=\"38\" />\n",
       "  <line x1=\"0\" y1=\"45\" x2=\"25\" y2=\"45\" />\n",
       "  <line x1=\"0\" y1=\"50\" x2=\"25\" y2=\"50\" />\n",
       "  <line x1=\"0\" y1=\"58\" x2=\"25\" y2=\"58\" />\n",
       "  <line x1=\"0\" y1=\"63\" x2=\"25\" y2=\"63\" />\n",
       "  <line x1=\"0\" y1=\"70\" x2=\"25\" y2=\"70\" />\n",
       "  <line x1=\"0\" y1=\"76\" x2=\"25\" y2=\"76\" />\n",
       "  <line x1=\"0\" y1=\"82\" x2=\"25\" y2=\"82\" />\n",
       "  <line x1=\"0\" y1=\"88\" x2=\"25\" y2=\"88\" />\n",
       "  <line x1=\"0\" y1=\"95\" x2=\"25\" y2=\"95\" />\n",
       "  <line x1=\"0\" y1=\"100\" x2=\"25\" y2=\"100\" />\n",
       "  <line x1=\"0\" y1=\"107\" x2=\"25\" y2=\"107\" />\n",
       "  <line x1=\"0\" y1=\"113\" x2=\"25\" y2=\"113\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,120.0 0.0,120.0\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >3</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">233312400</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<values, shape=(233312400, 3), dtype=float64, chunksize=(2379761, 3), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_csv('gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.csv', storage_options={'token':token}, assume_missing=True)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main loop, pulled from (ec_20_abernathy_etal/cloud_storage.ipynb). A few modifications have been made, but most notably we also want to measure the connection time to the cloud server. These times will be different depending on the connection method made, and in this case we are using the dask.dataframe.read_csv(. . .) function. This shouldn't matter if we are purely measuring access times, but it is important when we use different connection methods in the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='csv')\n",
    "\n",
    "for nworkers in [1, 3, 4]:\n",
    "    cluster.scale(nworkers)\n",
    "    time.sleep(10)\n",
    "    client.wait_for_workers(nworkers)\n",
    "    print(nworkers)\n",
    "    with diag_timer.time(nthreads=total_nthreads(), ncores=total_ncores(), nworkers=total_workers(), connectTime=connectTime, **diag_kwargs):\n",
    "        future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "        dask.compute(future, retries=5)\n",
    "    del future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nthreads</th>\n",
       "      <th>ncores</th>\n",
       "      <th>nworkers</th>\n",
       "      <th>connectTime</th>\n",
       "      <th>nbytes</th>\n",
       "      <th>chunksize</th>\n",
       "      <th>format</th>\n",
       "      <th>runtime</th>\n",
       "      <th>throughput_Mbps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.573894</td>\n",
       "      <td>5599497600</td>\n",
       "      <td>57114264</td>\n",
       "      <td>csv</td>\n",
       "      <td>73.443891</td>\n",
       "      <td>76.241843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.573894</td>\n",
       "      <td>5599497600</td>\n",
       "      <td>57114264</td>\n",
       "      <td>csv</td>\n",
       "      <td>31.221264</td>\n",
       "      <td>179.348843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.573894</td>\n",
       "      <td>5599497600</td>\n",
       "      <td>57114264</td>\n",
       "      <td>csv</td>\n",
       "      <td>26.074355</td>\n",
       "      <td>214.751144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nthreads  ncores  nworkers  connectTime      nbytes  chunksize format  \\\n",
       "0         2       2         1     0.573894  5599497600   57114264    csv   \n",
       "1         6       6         3     0.573894  5599497600   57114264    csv   \n",
       "2         8       8         4     0.573894  5599497600   57114264    csv   \n",
       "\n",
       "     runtime  throughput_Mbps  \n",
       "0  73.443891        76.241843  \n",
       "1  31.221264       179.348843  \n",
       "2  26.074355       214.751144  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = diag_timer.dataframe()\n",
    "df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "del da\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 6.95 GiB </td>\n",
       "                        <td> 72.62 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (233312400, 4) </td>\n",
       "                        <td> (2379761, 4) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 208 Tasks </td>\n",
       "                        <td> 104 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float64 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"5\" x2=\"25\" y2=\"5\" />\n",
       "  <line x1=\"0\" y1=\"11\" x2=\"25\" y2=\"11\" />\n",
       "  <line x1=\"0\" y1=\"19\" x2=\"25\" y2=\"19\" />\n",
       "  <line x1=\"0\" y1=\"24\" x2=\"25\" y2=\"24\" />\n",
       "  <line x1=\"0\" y1=\"32\" x2=\"25\" y2=\"32\" />\n",
       "  <line x1=\"0\" y1=\"37\" x2=\"25\" y2=\"37\" />\n",
       "  <line x1=\"0\" y1=\"44\" x2=\"25\" y2=\"44\" />\n",
       "  <line x1=\"0\" y1=\"50\" x2=\"25\" y2=\"50\" />\n",
       "  <line x1=\"0\" y1=\"57\" x2=\"25\" y2=\"57\" />\n",
       "  <line x1=\"0\" y1=\"63\" x2=\"25\" y2=\"63\" />\n",
       "  <line x1=\"0\" y1=\"70\" x2=\"25\" y2=\"70\" />\n",
       "  <line x1=\"0\" y1=\"76\" x2=\"25\" y2=\"76\" />\n",
       "  <line x1=\"0\" y1=\"82\" x2=\"25\" y2=\"82\" />\n",
       "  <line x1=\"0\" y1=\"88\" x2=\"25\" y2=\"88\" />\n",
       "  <line x1=\"0\" y1=\"95\" x2=\"25\" y2=\"95\" />\n",
       "  <line x1=\"0\" y1=\"100\" x2=\"25\" y2=\"100\" />\n",
       "  <line x1=\"0\" y1=\"107\" x2=\"25\" y2=\"107\" />\n",
       "  <line x1=\"0\" y1=\"113\" x2=\"25\" y2=\"113\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,120.0 0.0,120.0\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >4</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">233312400</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<values, shape=(233312400, 4), dtype=float64, chunksize=(2379761, 4), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_csv('gs://cloud-data-benchmarks/csvpartitions/*', storage_options={'token':token}, assume_missing=True)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='part_csv')\n",
    "\n",
    "for nworkers in [1, 3, 4]:\n",
    "    cluster.scale(nworkers)\n",
    "    time.sleep(10)\n",
    "    client.wait_for_workers(nworkers)\n",
    "    print(nworkers)\n",
    "    with diag_timer.time(nthreads=total_nthreads(), ncores=total_ncores(), nworkers=total_workers(), connectTime=connectTime, **diag_kwargs):\n",
    "        future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "        dask.compute(future, retries=5)\n",
    "    del future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nthreads</th>\n",
       "      <th>ncores</th>\n",
       "      <th>nworkers</th>\n",
       "      <th>connectTime</th>\n",
       "      <th>nbytes</th>\n",
       "      <th>chunksize</th>\n",
       "      <th>format</th>\n",
       "      <th>runtime</th>\n",
       "      <th>throughput_Mbps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.573894</td>\n",
       "      <td>5599497600</td>\n",
       "      <td>57114264</td>\n",
       "      <td>csv</td>\n",
       "      <td>73.443891</td>\n",
       "      <td>101.655790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.573894</td>\n",
       "      <td>5599497600</td>\n",
       "      <td>57114264</td>\n",
       "      <td>csv</td>\n",
       "      <td>31.221264</td>\n",
       "      <td>239.131791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.573894</td>\n",
       "      <td>5599497600</td>\n",
       "      <td>57114264</td>\n",
       "      <td>csv</td>\n",
       "      <td>26.074355</td>\n",
       "      <td>286.334859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8.429575</td>\n",
       "      <td>7465996800</td>\n",
       "      <td>76152352</td>\n",
       "      <td>part_csv</td>\n",
       "      <td>81.249845</td>\n",
       "      <td>91.889366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>8.429575</td>\n",
       "      <td>7465996800</td>\n",
       "      <td>76152352</td>\n",
       "      <td>part_csv</td>\n",
       "      <td>35.104092</td>\n",
       "      <td>212.681668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>8.429575</td>\n",
       "      <td>7465996800</td>\n",
       "      <td>76152352</td>\n",
       "      <td>part_csv</td>\n",
       "      <td>32.683390</td>\n",
       "      <td>228.433976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nthreads  ncores  nworkers  connectTime      nbytes  chunksize    format  \\\n",
       "0         2       2         1     0.573894  5599497600   57114264       csv   \n",
       "1         6       6         3     0.573894  5599497600   57114264       csv   \n",
       "2         8       8         4     0.573894  5599497600   57114264       csv   \n",
       "3         2       2         1     8.429575  7465996800   76152352  part_csv   \n",
       "4         6       6         3     8.429575  7465996800   76152352  part_csv   \n",
       "5         8       8         4     8.429575  7465996800   76152352  part_csv   \n",
       "\n",
       "     runtime  throughput_Mbps  \n",
       "0  73.443891       101.655790  \n",
       "1  31.221264       239.131791  \n",
       "2  26.074355       286.334859  \n",
       "3  81.249845        91.889366  \n",
       "4  35.104092       212.681668  \n",
       "5  32.683390       228.433976  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = diag_timer.dataframe()\n",
    "df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "del da\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetCDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the XArray documentation about reading and writing files [here](https://docs.xarray.dev/en/stable/user-guide/io.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "data = intake.open_netcdf('gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.nc', chunks=\"auto\").to_dask()\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "data = data.to_array() # Changes from DataSet to DataArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "da = data.data # Retrieves raw values from wrapped Xarray DataArray object\n",
    "da = dsa.rechunk(da, chunks={0:\"auto\", 1:5436, 2:\"auto\"})\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='NetCDF')\n",
    "\n",
    "for nworkers in [1, 3, 4]:\n",
    "    cluster.scale(nworkers)\n",
    "    time.sleep(10)\n",
    "    client.wait_for_workers(nworkers)\n",
    "    print(nworkers)\n",
    "    with diag_timer.time(nthreads=total_nthreads(), ncores=total_ncores(), nworkers=total_workers(), connectTime=connectTime, **diag_kwargs):\n",
    "        future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "        dask.compute(future, retries=5)\n",
    "    del future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = diag_timer.dataframe()\n",
    "df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "del da\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughput will be tested in two different ways:\n",
    "* A CSV file was partitioned into a DataFrame by Dask, and 1 parquet file was written per DataFrame partition. All of these files will be read into the null storage target\n",
    "* A single parquet file containing all of the original CSV information will be read into the null storage target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 5.21 GiB </td>\n",
       "                        <td> 54.47 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (233312400, 3) </td>\n",
       "                        <td> (2379761, 3) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 208 Tasks </td>\n",
       "                        <td> 104 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float64 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"5\" x2=\"25\" y2=\"5\" />\n",
       "  <line x1=\"0\" y1=\"11\" x2=\"25\" y2=\"11\" />\n",
       "  <line x1=\"0\" y1=\"19\" x2=\"25\" y2=\"19\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"25\" y2=\"25\" />\n",
       "  <line x1=\"0\" y1=\"32\" x2=\"25\" y2=\"32\" />\n",
       "  <line x1=\"0\" y1=\"38\" x2=\"25\" y2=\"38\" />\n",
       "  <line x1=\"0\" y1=\"45\" x2=\"25\" y2=\"45\" />\n",
       "  <line x1=\"0\" y1=\"50\" x2=\"25\" y2=\"50\" />\n",
       "  <line x1=\"0\" y1=\"58\" x2=\"25\" y2=\"58\" />\n",
       "  <line x1=\"0\" y1=\"63\" x2=\"25\" y2=\"63\" />\n",
       "  <line x1=\"0\" y1=\"70\" x2=\"25\" y2=\"70\" />\n",
       "  <line x1=\"0\" y1=\"76\" x2=\"25\" y2=\"76\" />\n",
       "  <line x1=\"0\" y1=\"82\" x2=\"25\" y2=\"82\" />\n",
       "  <line x1=\"0\" y1=\"88\" x2=\"25\" y2=\"88\" />\n",
       "  <line x1=\"0\" y1=\"95\" x2=\"25\" y2=\"95\" />\n",
       "  <line x1=\"0\" y1=\"100\" x2=\"25\" y2=\"100\" />\n",
       "  <line x1=\"0\" y1=\"107\" x2=\"25\" y2=\"107\" />\n",
       "  <line x1=\"0\" y1=\"113\" x2=\"25\" y2=\"113\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,120.0 0.0,120.0\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >3</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">233312400</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<values, shape=(233312400, 3), dtype=float64, chunksize=(2379761, 3), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_parquet(\"gs://cloud-data-benchmarks/parquetpartitions/*\")\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='part_parquet')\n",
    "\n",
    "for nworkers in [1, 3, 4]:\n",
    "    cluster.scale(nworkers)\n",
    "    time.sleep(10)\n",
    "    client.wait_for_workers(nworkers)\n",
    "    print(nworkers)\n",
    "    with diag_timer.time(nthreads=total_nthreads(), ncores=total_ncores(), nworkers=total_workers(), connectTime=connectTime, **diag_kwargs):\n",
    "        future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "        dask.compute(future, retries=5)\n",
    "    del future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nthreads</th>\n",
       "      <th>ncores</th>\n",
       "      <th>nworkers</th>\n",
       "      <th>connectTime</th>\n",
       "      <th>nbytes</th>\n",
       "      <th>chunksize</th>\n",
       "      <th>format</th>\n",
       "      <th>runtime</th>\n",
       "      <th>throughput_Mbps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.573894</td>\n",
       "      <td>5599497600</td>\n",
       "      <td>57114264</td>\n",
       "      <td>csv</td>\n",
       "      <td>73.443891</td>\n",
       "      <td>76.241843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.573894</td>\n",
       "      <td>5599497600</td>\n",
       "      <td>57114264</td>\n",
       "      <td>csv</td>\n",
       "      <td>31.221264</td>\n",
       "      <td>179.348843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.573894</td>\n",
       "      <td>5599497600</td>\n",
       "      <td>57114264</td>\n",
       "      <td>csv</td>\n",
       "      <td>26.074355</td>\n",
       "      <td>214.751144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8.429575</td>\n",
       "      <td>7465996800</td>\n",
       "      <td>76152352</td>\n",
       "      <td>part_csv</td>\n",
       "      <td>81.249845</td>\n",
       "      <td>68.917025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>8.429575</td>\n",
       "      <td>7465996800</td>\n",
       "      <td>76152352</td>\n",
       "      <td>part_csv</td>\n",
       "      <td>35.104092</td>\n",
       "      <td>159.511251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>8.429575</td>\n",
       "      <td>7465996800</td>\n",
       "      <td>76152352</td>\n",
       "      <td>part_csv</td>\n",
       "      <td>32.683390</td>\n",
       "      <td>171.325482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.407014</td>\n",
       "      <td>5599497600</td>\n",
       "      <td>57114264</td>\n",
       "      <td>part_parquet</td>\n",
       "      <td>31.959592</td>\n",
       "      <td>175.205540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2.407014</td>\n",
       "      <td>5599497600</td>\n",
       "      <td>57114264</td>\n",
       "      <td>part_parquet</td>\n",
       "      <td>12.126901</td>\n",
       "      <td>461.741836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2.407014</td>\n",
       "      <td>5599497600</td>\n",
       "      <td>57114264</td>\n",
       "      <td>part_parquet</td>\n",
       "      <td>9.171095</td>\n",
       "      <td>610.559303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nthreads  ncores  nworkers  connectTime      nbytes  chunksize  \\\n",
       "0         2       2         1     0.573894  5599497600   57114264   \n",
       "1         6       6         3     0.573894  5599497600   57114264   \n",
       "2         8       8         4     0.573894  5599497600   57114264   \n",
       "3         2       2         1     8.429575  7465996800   76152352   \n",
       "4         6       6         3     8.429575  7465996800   76152352   \n",
       "5         8       8         4     8.429575  7465996800   76152352   \n",
       "6         2       2         1     2.407014  5599497600   57114264   \n",
       "7         6       6         3     2.407014  5599497600   57114264   \n",
       "8         8       8         4     2.407014  5599497600   57114264   \n",
       "\n",
       "         format    runtime  throughput_Mbps  \n",
       "0           csv  73.443891        76.241843  \n",
       "1           csv  31.221264       179.348843  \n",
       "2           csv  26.074355       214.751144  \n",
       "3      part_csv  81.249845        68.917025  \n",
       "4      part_csv  35.104092       159.511251  \n",
       "5      part_csv  32.683390       171.325482  \n",
       "6  part_parquet  31.959592       175.205540  \n",
       "7  part_parquet  12.126901       461.741836  \n",
       "8  part_parquet   9.171095       610.559303  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = diag_timer.dataframe()\n",
    "df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "del da\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating size of array\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.worker - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker.html#memtrim for more information. -- Unmanaged memory: 5.24 GiB -- Worker memory limit: 7.35 GiB\n",
      "distributed.worker - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker.html#memtrim for more information. -- Unmanaged memory: 5.25 GiB -- Worker memory limit: 7.35 GiB\n",
      "distributed.worker - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker.html#memtrim for more information. -- Unmanaged memory: 5.57 GiB -- Worker memory limit: 7.35 GiB\n",
      "distributed.worker - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker.html#memtrim for more information. -- Unmanaged memory: 5.24 GiB -- Worker memory limit: 7.35 GiB\n",
      "distributed.worker - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker.html#memtrim for more information. -- Unmanaged memory: 5.50 GiB -- Worker memory limit: 7.35 GiB\n",
      "distributed.worker - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker.html#memtrim for more information. -- Unmanaged memory: 5.24 GiB -- Worker memory limit: 7.35 GiB\n",
      "distributed.worker - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker.html#memtrim for more information. -- Unmanaged memory: 5.24 GiB -- Worker memory limit: 7.35 GiB\n",
      "distributed.worker - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker.html#memtrim for more information. -- Unmanaged memory: 5.24 GiB -- Worker memory limit: 7.35 GiB\n",
      "distributed.worker - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker.html#memtrim for more information. -- Unmanaged memory: 5.79 GiB -- Worker memory limit: 7.35 GiB\n",
      "distributed.worker - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker.html#memtrim for more information. -- Unmanaged memory: 5.84 GiB -- Worker memory limit: 7.35 GiB\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_parquet(\"gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.parquet\")\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "\n",
    "# Memory Problem between this print statements, similar to the NetCDF issue\n",
    "print(\"Calculating size of array\")\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "print(\"Size of array has been calculated\")\n",
    "\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='parquet')\n",
    "\n",
    "for nworkers in [1, 3, 4]:\n",
    "    cluster.scale(nworkers)\n",
    "    time.sleep(10)\n",
    "    client.wait_for_workers(nworkers)\n",
    "    print(nworkers)\n",
    "    with diag_timer.time(nthreads=total_nthreads(), ncores=total_ncores(), nworkers=total_workers(), connectTime=connectTime, **diag_kwargs):\n",
    "        future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "        dask.compute(future, retries=5)\n",
    "    del future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = diag_timer.dataframe()\n",
    "df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "del da\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrayNotFoundError",
     "evalue": "array not found at path %r' 'ETOPO1_Ice_g_gmt4.zarr/z1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/site-packages/fsspec/mapping.py:135\u001b[0m, in \u001b[0;36mFSMap.__getitem__\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing_exceptions:\n",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/site-packages/fsspec/asyn.py:85\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/site-packages/fsspec/asyn.py:65\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/site-packages/fsspec/asyn.py:25\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/site-packages/fsspec/asyn.py:397\u001b[0m, in \u001b[0;36mAsyncFileSystem._cat\u001b[0;34m(self, path, recursive, on_error, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ex:\n\u001b[0;32m--> 397\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28mlen\u001b[39m(paths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m paths[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strip_protocol(path)\n\u001b[1;32m    402\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/asyncio/tasks.py:442\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout, loop)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/site-packages/gcsfs/core.py:768\u001b[0m, in \u001b[0;36mGCSFileSystem._cat_file\u001b[0;34m(self, path, start, end)\u001b[0m\n\u001b[1;32m    767\u001b[0m     head \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 768\u001b[0m headers, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, u2, headers\u001b[38;5;241m=\u001b[39mhead)\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/site-packages/gcsfs/core.py:392\u001b[0m, in \u001b[0;36mGCSFileSystem._call\u001b[0;34m(self, method, path, json_out, info_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 392\u001b[0m status, headers, info, contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    393\u001b[0m     method, path, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    394\u001b[0m )\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m json_out:\n",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/site-packages/decorator.py:221\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    220\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/site-packages/gcsfs/retry.py:115\u001b[0m, in \u001b[0;36mretry_request\u001b[0;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mmin\u001b[39m(random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (retry \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m32\u001b[39m))\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    117\u001b[0m     HttpError,\n\u001b[1;32m    118\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     aiohttp\u001b[38;5;241m.\u001b[39mclient_exceptions\u001b[38;5;241m.\u001b[39mClientError,\n\u001b[1;32m    122\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/site-packages/gcsfs/core.py:384\u001b[0m, in \u001b[0;36mGCSFileSystem._request\u001b[0;34m(self, method, path, headers, json, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m r\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 384\u001b[0m \u001b[43mvalidate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m status, headers, info, contents\n",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/site-packages/gcsfs/retry.py:84\u001b[0m, in \u001b[0;36mvalidate_response\u001b[0;34m(status, content, path, args)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[1;32m     86\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: https://storage.googleapis.com/download/storage/v1/b/cloud-data-benchmarks/o/ETOPO1_Ice_g_gm4.zarr%2FETOPO1_Ice_g_gmt4.zarr%2Fz1%2F.zarray?alt=media",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/site-packages/zarr/core.py:206\u001b[0m, in \u001b[0;36mArray._load_metadata_nosync\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m     mkey \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key_prefix \u001b[38;5;241m+\u001b[39m array_meta_key\n\u001b[0;32m--> 206\u001b[0m     meta_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/site-packages/zarr/storage.py:545\u001b[0m, in \u001b[0;36mKVStore.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mutable_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/site-packages/fsspec/mapping.py:139\u001b[0m, in \u001b[0;36mFSMap.__getitem__\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ETOPO1_Ice_g_gmt4.zarr/z1/.zarray'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mArrayNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m tic1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m da \u001b[38;5;241m=\u001b[39m \u001b[43mdsa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_zarr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgs://cloud-data-benchmarks/ETOPO1_Ice_g_gm4.zarr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mETOPO1_Ice_g_gmt4.zarr/z1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m toc1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m connectTime \u001b[38;5;241m=\u001b[39m toc1 \u001b[38;5;241m-\u001b[39m tic1\n",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/site-packages/dask/array/core.py:3384\u001b[0m, in \u001b[0;36mfrom_zarr\u001b[0;34m(url, component, storage_options, chunks, name, inline_array, **kwargs)\u001b[0m\n\u001b[1;32m   3382\u001b[0m         url \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(url)\n\u001b[1;32m   3383\u001b[0m     mapper \u001b[38;5;241m=\u001b[39m get_mapper(url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstorage_options)\n\u001b[0;32m-> 3384\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3386\u001b[0m     mapper \u001b[38;5;241m=\u001b[39m url\n",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/site-packages/zarr/core.py:180\u001b[0m, in \u001b[0;36mArray.__init__\u001b[0;34m(self, store, path, read_only, chunk_store, synchronizer, cache_metadata, cache_attrs, partial_decompress, write_empty_chunks)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_empty_chunks \u001b[38;5;241m=\u001b[39m write_empty_chunks\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# initialize metadata\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# initialize attributes\u001b[39;00m\n\u001b[1;32m    183\u001b[0m akey \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key_prefix \u001b[38;5;241m+\u001b[39m attrs_key\n",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/site-packages/zarr/core.py:197\u001b[0m, in \u001b[0;36mArray._load_metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"(Re)load metadata from store.\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_synchronizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_metadata_nosync\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     mkey \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key_prefix \u001b[38;5;241m+\u001b[39m array_meta_key\n",
      "File \u001b[0;32m~/miniconda3/envs/jgreen_daskMLtut/lib/python3.9/site-packages/zarr/core.py:208\u001b[0m, in \u001b[0;36mArray._load_metadata_nosync\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m     meta_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store[mkey]\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArrayNotFoundError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m \n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# decode and store metadata as instance members\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store\u001b[38;5;241m.\u001b[39m_metadata_class\u001b[38;5;241m.\u001b[39mdecode_array_metadata(meta_bytes)\n",
      "\u001b[0;31mArrayNotFoundError\u001b[0m: array not found at path %r' 'ETOPO1_Ice_g_gmt4.zarr/z1'"
     ]
    }
   ],
   "source": [
    "tic1 = time.time()\n",
    "da = dsa.from_zarr('gs://cloud-data-benchmarks/ETOPO1_Ice_g_gm4.zarr', component='ETOPO1_Ice_g_gmt4.zarr/z1')\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='part_parquet')\n",
    "\n",
    "for nworkers in [1, 3, 4]:\n",
    "    cluster.scale(nworkers)\n",
    "    time.sleep(10)\n",
    "    client.wait_for_workers(nworkers)\n",
    "    print(nworkers)\n",
    "    with diag_timer.time(nthreads=total_nthreads(), ncores=total_ncores(), nworkers=total_workers(), connectTime=connectTime, **diag_kwargs):\n",
    "        future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "        dask.compute(future, retries=5)\n",
    "    del future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = diag_timer.dataframe()\n",
    "df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "del da\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH ubuntu@10.128.0.150 jacob-lab-jgreen_daskMLtut",
   "language": "",
   "name": "rik_ssh_ubuntu_10_128_0_150_jacoblabjgreen_daskmltut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
