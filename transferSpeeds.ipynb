{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Parallel Data Read Speeds with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This benchmarking was inspired by the [Abernathey et al. (2021)](https://www.computer.org/csdl/magazine/cs/2021/02/09354557/1reXu4gJjri) paper, which includes a detailed analysis and discussion of cloud-native big scientific data. The paper is much wider in scope than this benchmarking, so it is recommended that a reader unacclimated with this subject matter first studies its information before going through this notebook.\n",
    "\n",
    "Unfortunately, due to the access token required to retrieve data the Google Cloud Storage Bucket used in this benchmarking, the public reader will not be able to run this notebook on their own. If you wish to download your own copy and alter the code, the test data from [NOAA's ETOPO1 Global Relief Model](https://www.ngdc.noaa.gov/mgg/global/relief/ETOPO1/data/ice_surface/grid_registered/netcdf/) can be found here. A file transformation notebook is also found within the repository, which documents my methods for converting from the original NetCDF format. Note that a CSV version was created outside of this project space.\n",
    "\n",
    "Different methods for accessing Google Cloud Storage are used--contigent on the file type of the data. Though discussed in further detail later on, I encourage the reader to read about the different file formats used in this benchmarking, as well as the access APIs. In many instances, data can be loaded directly into Dask objects without further manipulation. Other libraries were also used to load data in, mainly due to the limitations of Dask or speed benefits of using another library.\n",
    "\n",
    "A significant portion of the code used in this benchmarking is directly taken from [Ryan Abernathey's demonstration notebook](https://github.com/earthcube2020/ec20_abernathey_etal/blob/master/cloud_storage.ipynb), and it is worth viewing before beginning this read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Client Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as dsa\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from contextlib import contextmanager\n",
    "import xarray as xr\n",
    "import intake\n",
    "import time\n",
    "import dask\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create this null storage object. To measure our throughput, all of the data will need to be accessed at a single time and can be achieved by storing the data into this null storage target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevNullStore:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __setitem__(*args, **kwargs):\n",
    "        pass\n",
    "\n",
    "null_store = DevNullStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Diagnostic Timer will keep track of data retrieval times and store them within a pandas dataframe for later processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagnosticTimer:\n",
    "    def __init__(self):\n",
    "        self.diagnostics = []\n",
    "        \n",
    "    @contextmanager\n",
    "    def time(self, **kwargs):\n",
    "        tic = time.time()\n",
    "        yield\n",
    "        toc = time.time()\n",
    "        kwargs[\"runtime\"] = toc - tic\n",
    "        self.diagnostics.append(kwargs)\n",
    "        \n",
    "    def dataframe(self):\n",
    "        return pd.DataFrame(self.diagnostics)\n",
    "    \n",
    "diag_timer = DiagnosticTimer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This naming function will keep track of our read throughput for each file type and make it easier to plot all of the cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name(fileType, daf): # Takes a string fileType input & dataframe type input for daf \n",
    "    globals()[f\"df_{fileType}\"] = daf\n",
    "    global df, da\n",
    "    del df, da\n",
    "    diag_timer.diagnostics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we are accessing the data from different formats, the core process will be the exact same. Dask is lazy by default, which means we will be using the previously defined null_store to measure throughput. Converting from DataFrames to Arrays, for example, will not affect the access speed because the data is not actually read from the source until we \"store\" it. So, these read speeds should be the same whether you are initially pointing to data using a dataframe or array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A token may or may not be needed depending on the permissions your specific Google Bucket requires.\n",
    "token = '/home/ubuntu/Cloud-Data-Transfer-Speed-Benchmarks/cloud-data-benchmarks.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about our cluster is also collected during the tests. Importantly, the number of parallel reads will correspond to the total number of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_nthreads():\n",
    "    return sum([v for v in client.nthreads().values()])\n",
    "\n",
    "def total_ncores():\n",
    "    return sum([v for v in client.ncores().values()])\n",
    "\n",
    "def total_workers():\n",
    "    return len(client.ncores())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the classic tabular data format. Students & professionals alike use this format due to its simplicity and flexibility bewteen applications. As you will see, this format has the slowest read times of all being tested but is included in the benchmarking given its widespread use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_csv('gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.csv', storage_options={'token':token}, assume_missing=True)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main loop. A few modifications have been made from Abernathey's original version, but most notably we want to measure the connection time to the data reference. This shouldn't matter if we are purely measuring access times, but it is important when we use different modules to connect to Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='csv')\n",
    "\n",
    "for nworkers in [1, 3, 4]:\n",
    "    cluster.scale(nworkers)\n",
    "    time.sleep(10)\n",
    "    client.wait_for_workers(nworkers)\n",
    "    print(nworkers)\n",
    "    with diag_timer.time(nthreads=total_nthreads(), ncores=total_ncores(), nworkers=total_workers(), connectTime=connectTime, **diag_kwargs):\n",
    "        future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "        dask.compute(future, retries=5)\n",
    "    del future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = diag_timer.dataframe()\n",
    "df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "name('csv', df)\n",
    "df_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While one CSV file is the most common way one will see data of this format presented, we are interested in determining if partitioning (splitting up) the data into file sizes automatically determined by Dask will speed up the read from cloud storage. The process is the exact same, with the exception of a minor amount of required preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_csv('gs://cloud-data-benchmarks/csvpartitions/*', storage_options={'token':token}, assume_missing=True)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='part_csv')\n",
    "\n",
    "for nworkers in [1, 3, 4]:\n",
    "    cluster.scale(nworkers)\n",
    "    time.sleep(10)\n",
    "    client.wait_for_workers(nworkers)\n",
    "    print(nworkers)\n",
    "    with diag_timer.time(nthreads=total_nthreads(), ncores=total_ncores(), nworkers=total_workers(), connectTime=connectTime, **diag_kwargs):\n",
    "        future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "        dask.compute(future, retries=5)\n",
    "    del future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = diag_timer.dataframe()\n",
    "df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "name('partcsv', df)\n",
    "df_partcsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetCDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NetCDF files are extremely common in geospatial and climate science, which is the original format of the test data used in this benchmarking. Data from these fields are commonly very large, with sizes only increasing as missions collecting samples at higher resolutions are conducted. The informed scientist should have a good grasp on the speed limitations of this format and the best ways to load in and perform computations within Python.\n",
    "\n",
    "We load the NetCDF using the intake library. This is a great way to load the data from GCS without directly using the gcsfs module. Depending on the NetCDF file contents, decoding the data using the engines included with xarray.open_dataset(...) can be troublesome. Read the XArray documentation about accessing files using xarray.open_dataset(...) [here](https://docs.xarray.dev/en/stable/user-guide/io.html).\n",
    "\n",
    "The intake module is much easier to use, and offers some additional functionality in the form of catalogs. Read about catalogs and their uses [here](https://intake.readthedocs.io/en/latest/catalog.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "data = intake.open_netcdf('gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.nc').to_dask()\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "data = data.to_array() # Changes from DataSet to DataArray\n",
    "da = data.data # Retrieves raw values from wrapped Xarray DataArray object\n",
    "da = dsa.from_array(da)\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encounter a large unmanaged memory warning in the dask.compute(future, retries=5) line, causing the data store to fail. Investigation into a solution is underway, but please do not hesistate to comment with any suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='NetCDF')\n",
    "\n",
    "for nworkers in [1, 3, 4]:\n",
    "    cluster.scale(nworkers)\n",
    "    time.sleep(10)\n",
    "    client.wait_for_workers(nworkers)\n",
    "    print(nworkers)\n",
    "    with diag_timer.time(nthreads=total_nthreads(), ncores=total_ncores(), nworkers=total_workers(), connectTime=connectTime, **diag_kwargs):\n",
    "        future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "        dask.compute(future, retries=5)\n",
    "    del future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = diag_timer.dataframe()\n",
    "df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "name('netcdf', df)\n",
    "df_netcdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughput will be tested in two different ways:\n",
    "* A CSV file was partitioned into a DataFrame by Dask, and 1 parquet file was written per DataFrame partition. All of these files will be read into the null storage target\n",
    "* A single parquet file containing all of the original CSV information will be read into the null storage target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_parquet(\"gs://cloud-data-benchmarks/parquetpartitions/*\")\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='part_parquet')\n",
    "\n",
    "for nworkers in [1, 3, 4]:\n",
    "    cluster.scale(nworkers)\n",
    "    time.sleep(10)\n",
    "    client.wait_for_workers(nworkers)\n",
    "    print(nworkers)\n",
    "    with diag_timer.time(nthreads=total_nthreads(), ncores=total_ncores(), nworkers=total_workers(), connectTime=connectTime, **diag_kwargs):\n",
    "        future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "        dask.compute(future, retries=5)\n",
    "    del future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = diag_timer.dataframe()\n",
    "df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "name('partparquet', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encounter a problem with large unmanaged memory when attempting to compute the size of DataFrame. Generally, it is recommended that parquet files be partitioned in advance to avoid this issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_parquet(\"gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.parquet\")\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "\n",
    "# Memory Problem between this print statements, similar to the NetCDF issue\n",
    "print(\"Calculating size of array\")\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "print(\"Size of array has been calculated\")\n",
    "\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='parquet')\n",
    "\n",
    "for nworkers in [1, 3, 4]:\n",
    "    cluster.scale(nworkers)\n",
    "    time.sleep(10)\n",
    "    client.wait_for_workers(nworkers)\n",
    "    print(nworkers)\n",
    "    with diag_timer.time(nthreads=total_nthreads(), ncores=total_ncores(), nworkers=total_workers(), connectTime=connectTime, **diag_kwargs):\n",
    "        future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "        dask.compute(future, retries=5)\n",
    "    del future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = diag_timer.dataframe()\n",
    "df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "name('parquet', df)\n",
    "df_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See <a href=\"https://www.programcreek.com/python/example/128207/dask.array.from_zarr#:~:text=.chunks)-,Example%205,-Project%3A%20napari\">this python example</a> of dask.array.from_zarr(...) for further understanding of the differences between a Zarr Array & Zarr Group.\n",
    "\n",
    "A Zarr Group is a very good choice of storage utility for multiscale data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zarr Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "da = dsa.from_zarr('gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.zarray')\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='zarr_array')\n",
    "\n",
    "for nworkers in [1, 3, 4]:\n",
    "    cluster.scale(nworkers)\n",
    "    time.sleep(10)\n",
    "    client.wait_for_workers(nworkers)\n",
    "    print(nworkers)\n",
    "    with diag_timer.time(nthreads=total_nthreads(), ncores=total_ncores(), nworkers=total_workers(), connectTime=connectTime, **diag_kwargs):\n",
    "        future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "        dask.compute(future, retries=5)\n",
    "    del future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = diag_timer.dataframe()\n",
    "df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "name('zarray', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zarr Hierachical Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xarray provides a fantasic tool in their API for opening Zarr groups. By consolidating the metadata upon creation of the group, the read speed increases considerably more than if it was left in its base state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "zarr_ds = xr.open_zarr(store='gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.zarr', storage_options={'token':token}, consolidated=True)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "darray = zarr_ds.to_array()\n",
    "da = darray.data\n",
    "\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='zarr_group')\n",
    "\n",
    "for nworkers in [1, 3, 4]:\n",
    "    cluster.scale(nworkers)\n",
    "    time.sleep(10)\n",
    "    client.wait_for_workers(nworkers)\n",
    "    print(nworkers)\n",
    "    with diag_timer.time(nthreads=total_nthreads(), ncores=total_ncores(), nworkers=total_workers(), connectTime=connectTime, **diag_kwargs):\n",
    "        future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "        dask.compute(future, retries=5)\n",
    "    del future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = diag_timer.dataframe()\n",
    "df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "name('zgroup', df)\n",
    "df_zgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_csv.plot(x='nworkers', y='throughput_Mbps', title='Cloud Data Read Speeds with Dask', kind='scatter', color='green')\n",
    "plt.grid(True)\n",
    "df_partcsv.plot(ax=ax, x='nworkers', y='throughput_Mbps', kind='scatter', color='limegreen')\n",
    "df_partparquet.plot(ax=ax, x='nworkers', y='throughput_Mbps', kind='scatter', color='blue')\n",
    "df_zarray.plot(ax=ax, x='nworkers', y='throughput_Mbps', kind='scatter', color='red')\n",
    "df_zgroup.plot(ax=ax, x='nworkers', y='throughput_Mbps', kind='scatter', \n",
    "                 xlabel='Number of Parallel Reads', ylabel='Throughput (Mbps)', color='darkred')\n",
    "plt.grid(True)\n",
    "plt.legend(['CSV', 'Partitioned CSV', 'Partitioned Parquet','Zarr Array', 'Zarr Group'], loc='upper left', title='Store Formats')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH ubuntu@10.128.0.150 jacob-lab-jgreen_daskMLtut",
   "language": "",
   "name": "rik_ssh_ubuntu_10_128_0_150_jacoblabjgreen_daskmltut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
