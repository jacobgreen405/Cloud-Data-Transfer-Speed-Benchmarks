{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Parallel Data Read Speeds with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This benchmarking was inspired by the [Abernathey et al. (2021)](https://www.computer.org/csdl/magazine/cs/2021/02/09354557/1reXu4gJjri) paper, which includes a detailed analysis and discussion of cloud-native big scientific data. It is recommended that a reader unacclimated with this subject matter first studies its information before going through this notebook.\n",
    "\n",
    "Unfortunately, due to the access token required to retrieve data the Google Cloud Storage Bucket used in this benchmarking, the public reader will not be able to run this notebook on their own. If you wish to download your own copy and alter the code, the test data from [NOAA's ETOPO1 Global Relief Model](https://www.ngdc.noaa.gov/mgg/global/relief/ETOPO1/data/ice_surface/grid_registered/netcdf/) can be found here. A file transformation notebook is also found within the repository, which documents my methods for converting from the original NetCDF format. Note that a CSV version was created outside of this project space using Generic Mapping Tools (GMT). A validation notebook has also been included in the repository to test each file type for the correct data contents.\n",
    "\n",
    "Different methods for accessing Google Cloud Storage are used--contigent on the file type of the data. Though discussed in further detail later on, I encourage the reader to read about the different file formats used in this benchmarking, as well as the access APIs. In many instances, data can be loaded directly into Dask objects without further manipulation. Other libraries were also used to load data in, mainly due to the limitations of Dask or speed benefits of using another library.\n",
    "\n",
    "A significant portion of the code used in this benchmarking is directly taken from [Ryan Abernathey's demonstration notebook](https://github.com/earthcube2020/ec20_abernathey_etal/blob/master/cloud_storage.ipynb), and it is worth viewing before beginning this read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **NetCDF3, which is the original format of the ETOPO1_Ice_g_gmt4.nc file, does not support internal chunking. If you choose to download the data from the previously provided link, it must first be converted into NetCDF4 in order to successfully run the entire notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Client Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as dsa\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from contextlib import contextmanager\n",
    "import xarray as xr\n",
    "import intake # must install intake-xarray\n",
    "import time\n",
    "import dask\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import pandas as pd\n",
    "from scipy.stats import sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(threads_per_worker=2)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create this null storage object. To measure our throughput, all of the data will need to be accessed at a single time and can be achieved by storing the data into this null storage target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevNullStore:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __setitem__(*args, **kwargs):\n",
    "        pass\n",
    "\n",
    "null_store = DevNullStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Diagnostic Timer will keep track of data retrieval times and store them within a Pandas DataFrame for later processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagnosticTimer:\n",
    "    def __init__(self):\n",
    "        self.diagnostics = []\n",
    "        self.names = []\n",
    "        \n",
    "    @contextmanager\n",
    "    def time(self, **kwargs):\n",
    "        tic = time.time()\n",
    "        yield\n",
    "        toc = time.time()\n",
    "        kwargs[\"runtime\"] = toc - tic\n",
    "        self.diagnostics.append(kwargs)\n",
    "        \n",
    "    def dataframe(self):\n",
    "        return pd.DataFrame(self.diagnostics)\n",
    "    \n",
    "diag_timer = DiagnosticTimer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This naming function will keep track of our read throughput for each file type and make it easier to plot all of the cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name(fileType, daf): # Takes a string fileType input & dataframe type input for daf \n",
    "    globals()[f\"df_{fileType}\"] = daf\n",
    "    diag_timer.names.append(globals()[f\"df_{fileType}\"])\n",
    "    \n",
    "    global df, da\n",
    "    del df, da\n",
    "    \n",
    "    diag_timer.diagnostics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about our cluster is also collected during the tests. Importantly, the number of parallel reads will correspond to the total number of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_nthreads():\n",
    "    return sum([v for v in client.nthreads().values()])\n",
    "\n",
    "def total_ncores():\n",
    "    return sum([v for v in client.ncores().values()])\n",
    "\n",
    "def total_workers():\n",
    "    return len(client.ncores())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main loop. A few modifications have been made from Abernathey's original version, but most notably we want to measure the connection time to the data reference. This shouldn't matter if we are purely measuring access times, but it is important when we use different modules to connect to Google Cloud Storage.\n",
    "\n",
    "The user also has the choice between running the normal loop -- which calculates throughput a single time for each file format -- or a nested loop, which will run through each file type multiple times and determine the average throughput & error. The nested loop will take much longer to run, but will allow for a more accurate analysis.\n",
    "\n",
    "**ONLY USE ONE OF THE FOLLOWING TWO SECTIONS AND RUN THE REST OF THE NOTEBOOK NORMALLY.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainLoop(da, diag_kwargs):\n",
    "    global max_workers, worker_step\n",
    "    nworkers = max_workers\n",
    "    while nworkers > 0:\n",
    "        cluster.scale(nworkers)\n",
    "        time.sleep(10)\n",
    "        client.wait_for_workers(nworkers)\n",
    "        print('Number of Workers:', nworkers)\n",
    "        with diag_timer.time(nworkers=total_workers(), nthreads=total_nthreads(), ncores=total_ncores(), **diag_kwargs):\n",
    "            future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "            dask.compute(future, retries=5)\n",
    "        del future   \n",
    "        nworkers -= worker_step\n",
    "        \n",
    "    df = diag_timer.dataframe()\n",
    "    df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nested Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the nested loop requires that you run the `errorCalc(...)` function. It will keep track of the standard error of mean for each file type for use in error bars in later plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = 5 # (integer) This number will correspond to the number of times each file format will be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorCalc(df):\n",
    "    global tests\n",
    "    errors = []\n",
    "    means = []\n",
    "    info = []\n",
    "    sample = df['throughput_Mbps']\n",
    "    for i in np.linspace(0, len(sample)-tests,int(len(sample)/tests), dtype='int'):\n",
    "        temp = sample[i:(i+4)]\n",
    "        errors.append(sem(temp))\n",
    "        means.append(temp.mean())\n",
    "        info.append(df.iloc[i, 0:7])\n",
    "        \n",
    "    df0 = pd.DataFrame(info, index=range(len(info)))\n",
    "    df0['throughput_Mbps'] = pd.DataFrame(means)\n",
    "    df0['errors'] = pd.DataFrame(errors)\n",
    "    return df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainLoop(da, diag_kwargs):\n",
    "    global tests, max_workers, worker_step\n",
    "    nworkers = max_workers\n",
    "    while nworkers > 0:\n",
    "        cluster.scale(nworkers)\n",
    "        time.sleep(10)\n",
    "        client.wait_for_workers(nworkers)\n",
    "        print('Number of Workers:', nworkers)\n",
    "        for i in range(tests): #Change range\n",
    "            with diag_timer.time(nworkers=total_workers(), nthreads=total_nthreads(), ncores=total_ncores(), **diag_kwargs):\n",
    "                future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "                dask.compute(future, retries=5)\n",
    "            del future   \n",
    "        nworkers -= worker_step\n",
    "        \n",
    "    df = diag_timer.dataframe()\n",
    "    df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "    df = errorCalc(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worker Range Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input the desired maximum workers and step size that the main loop will use to iterate. Note that they must be integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_workers = 8 # (integer) This corresponds to the maximum amount of workers that will be used\n",
    "worker_step = 1 # (integer) Will scale down the cluster by this amount through each iteration of the main loop\n",
    "root = 'gs://cloud-data-benchmarks/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we are accessing the data from different formats, the core process will be the exact same. Dask is lazy by default, which means we will be using the previously defined null_store to measure throughput. Converting from DataFrames to Arrays, for example, will not affect the access speed because the data is not actually read from the source until we \"store\" it. So, these read speeds should be the same whether you are initially pointing to data using a dataframe or array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the classic tabular data format. Students & professionals alike use this format due to its simplicity and flexibility bewteen applications. As you will see, this format has the slowest read times of all being tested but is included in the benchmarking given its widespread use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_csv(root + 'ETOPO1_Ice_g_gmt4.csv',assume_missing=True, names=['lon', 'lat', 'z'])\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='CSV', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop(da, diag_kwargs)\n",
    "name('csv', df)\n",
    "df_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While one CSV file is the most common way one will see data of this format presented, we are interested in determining if partitioning (splitting up) the data into file sizes automatically determined by Dask will speed up the read from cloud storage. The process is the exact same, with the exception of a minor amount of required preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_csv(root + 'csvpartitions/*', assume_missing=True, names=['lon', 'lat', 'z'])\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='Partitioned CSV', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop(da, diag_kwargs)\n",
    "name('partcsv', df)\n",
    "df_partcsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughput will be tested in two different ways:\n",
    "* **Multiple Files:** A CSV file was partitioned into a DataFrame by Dask, and 1 parquet file was written per DataFrame partition. All of these files will be read into the null storage target\n",
    "* **Single File:** A single parquet file containing all of the original CSV information will be read into the null storage target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_parquet(root + 'parquetpartitions/*')\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='Partitioned Parquet', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop(da, diag_kwargs)\n",
    "name('partparquet', df)\n",
    "df_partparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encounter a problem with large unmanaged memory when attempting to compute the size of DataFrame. This format is used here as an example of a file format that will not work in parallel, and should **only** be run to demonstrate a use case that does not function.\n",
    "\n",
    "Generally, it is recommended that parquet files be partitioned in advance to avoid this issue. Dask currently creates DataFrame partitions based on the number of files stored within the parquet file path. Having one large file goes against the point of switching to parquet files in the first place because Dask will not be able to partition the data for parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_parquet(root + 'ETOPO1_Ice_g_gmt4.parquet')\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "# Memory Problem between these print statements, similar to the NetCDF issue\n",
    "print(\"Calculating size of array\")\n",
    "da = df0.to_dask_array()\n",
    "print(\"Size of array has been calculated\")\n",
    "\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='Parquet', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop(da, diag_kwargs)\n",
    "name('parquet', df)\n",
    "df_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetCDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NetCDF files are extremely common in geospatial and climate science, which is the original format of the test data used in this benchmarking. Data from these fields are commonly very large, with sizes only increasing as missions collecting samples at higher resolutions are conducted. The informed scientist should have a good grasp on the speed limitations of this format and the best ways to load in and perform computations within Python.\n",
    "\n",
    "We load the NetCDF using the `intake` library. This is a great way to load the data from GCS without directly using the `gcsfs` module. Depending on the NetCDF file contents, decoding the data using the engines included with `xarray.open_dataset(...)` can be troublesome. Read the XArray documentation about accessing files using `xarray.open_dataset(...)` [here](https://docs.xarray.dev/en/stable/user-guide/io.html).\n",
    "\n",
    "The `intake` module is much easier to use, and offers some additional functionality in the form of catalogs. Read about catalogs and their uses [here](https://intake.readthedocs.io/en/latest/catalog.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "data = intake.open_netcdf(root + 'ETOPO1_Ice_g_gmt4.nc').to_dask()\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "data = data.to_array()\n",
    "da = data.data # Retrieves raw values from wrapped Xarray DataArray object\n",
    "da = dsa.from_array(da)\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "del data\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='NetCDF', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop(da, diag_kwargs)\n",
    "name('netcdf', df)\n",
    "df_netcdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zarr Groups/Arrays are cloud-native gridded data formats, making them very useful for converting NetCDF files. It is known that there will be a large speedup with cloud read speed compared to the legacy data formats, but the performance differences between a Group/Array is important to note.\n",
    "\n",
    "See <a href=\"https://www.programcreek.com/python/example/128207/dask.array.from_zarr#:~:text=.chunks)-,Example%205,-Project%3A%20napari\">this python example</a> of `dask.array.from_zarr(...)` for further understanding of the differences between a Zarr Array & Zarr Group. A Zarr Group is a very good choice of storage utility for multiscale data due to its hierarchical paths in an object storage system -- this allows many different data sets to be stored in subdirectories under a larger Zarr store directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zarr Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "da = dsa.from_zarr(root + 'ETOPO1_Ice_g_gmt4.zarray')\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='Zarr Array', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop(da, diag_kwargs)\n",
    "name('zarray', df)\n",
    "df_zarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zarr Hierachical Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xarray provides a fantasic tool in their API for opening Zarr groups. By consolidating the metadata upon creation of the group, the read speed increases considerably more than if it was left in its base state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "zarr_ds = xr.open_zarr(store= root + 'ETOPO1_Ice_g_gmt4.zarr', consolidated=True)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "darray = zarr_ds.to_array()\n",
    "da = darray.data\n",
    "del zarr_ds, darray\n",
    "\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='Zarr Group', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop(da, diag_kwargs)\n",
    "name('zgroup', df)\n",
    "df_zgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If using exactly 4 different amounts of workers, the plot colors will enter a greyscale from. This is due to the color array containing length 4 for each file type, which matches the number of data points that will be plotted for each. In large scale testing this will not be a problem, but could be a problem when running this notebook on a local cluster.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorPlot(df, c):\n",
    "    x = df['nworkers']\n",
    "    y = df['throughput_Mbps']\n",
    "    error = df['errors']\n",
    "    plt.errorbar(x, y, yerr=error, color=c, fmt='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = cm.rainbow(np.linspace(0,1,len(diag_timer.names)))\n",
    "legend = []\n",
    "\n",
    "for i in range(len(diag_timer.names)):\n",
    "    legend.append(diag_timer.names[i]['format'][1])\n",
    "    c = color[i,:]\n",
    "    \n",
    "    if i==0:\n",
    "        ax = diag_timer.names[i].plot(x='nworkers', y='throughput_Mbps', title='Cloud Data Read Speeds with Dask',\n",
    "                                      kind='line', color=c, marker='o')\n",
    "        try:\n",
    "            errorPlot(diag_timer.names[i], c)\n",
    "        except:\n",
    "            pass\n",
    "        else:\n",
    "            errorPlot(diag_timer.names[i], c)\n",
    "    \n",
    "    elif i==(len(diag_timer.names)-1):\n",
    "        diag_timer.names[i].plot(x='nworkers', y='throughput_Mbps', kind='line', color=c, ax=ax, \n",
    "                                 xlabel='Number of Parallel Reads', ylabel='Throughput (Mbps)', marker='o')\n",
    "        try:\n",
    "            errorPlot(diag_timer.names[i], c)\n",
    "        except:\n",
    "            print('Plotting Normally')\n",
    "        else:\n",
    "            print('Plotting with Error Bars')\n",
    "            errorPlot(diag_timer.names[i], c)\n",
    "        \n",
    "        plt.grid(True)\n",
    "        plt.legend(legend, bbox_to_anchor=[1.25, 0.5], loc='center', title='Store Formats')\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        #plt.yscale('symlog') ACTIVATE THIS LINE IF YOU ARE USING A LARGE AMOUNT OF WORKERS\n",
    "    \n",
    "    else:\n",
    "        diag_timer.names[i].plot(x='nworkers', y='throughput_Mbps', kind='line', color=c, ax=ax, marker='o')\n",
    "        try:\n",
    "            errorPlot(diag_timer.names[i], c)\n",
    "        except:\n",
    "            pass\n",
    "        else:\n",
    "            errorPlot(diag_timer.names[i], c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH ubuntu@10.128.0.150 jacob-lab-jgreen_daskMLtut",
   "language": "",
   "name": "rik_ssh_ubuntu_10_128_0_150_jacoblabjgreen_daskmltut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
